{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a importar las librerías necesarias para la tarea, ya se explicará más adelante el uso de _combinations_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybalu.feature_extraction import lbp_features\n",
    "from pybalu.feature_selection import clean, sfs\n",
    "from pybalu.feature_transformation import normalize\n",
    "from pybalu.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = [i for i in listdir('faces_ARLQ') if i.endswith('png')]\n",
    "usable_faces = [i for i in faces if int(i[9:11]) < 8]\n",
    "odd_faces_names = [i for i in usable_faces if int(i[5:8]) % 2]\n",
    "even_faces_names = [i for i in usable_faces if not int(i[5:8]) % 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras cargar todos los nombres de las imágenes en la carpeta _faces_ARLQ_ se procede a dividirlas entre pares e impares, y las labels se trasladan para que las pares no vayan de 1 a 100 con salto de 2, sino que de 0 a 49, correlativamente, lo anterior debido a una limitación de la implementación de SFS en pybalu que lo exigía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_names = [i for i in odd_faces_names if int(i[9:11]) > 1]\n",
    "testing_names = [i for i in odd_faces_names if int(i[9:11]) <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [imread(f'faces_ARLQ/{i}') for i in training_names]\n",
    "y_training = np.array([int(i[5:8]) // 2 for i in training_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = [imread(f'faces_ARLQ/{i}') for i in testing_names]\n",
    "y_testing = np.array([int(i[5:8]) // 2 for i in testing_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se probaron distintas combinaciones de _hdiv_ y _vdiv_, ambos entre los valores de 1 y 7. En un principio se probó con las más grandes (7x7) pero eran excesivamente lentas de procesar en el SFS, y se optó por probar las combinaciones extremas y más acotadas, esto es 5x1 y 1x5 iterando hasta 1x1. Si bien en varios experimentos se obtuvo un buen rendimiento (100% o muy cercano), el que mejor pondera tiempo de ejecución y rendimiento es el siguiente: 4x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiv = 4\n",
    "vdiv = 2\n",
    "training_features = np.array([lbp_features(i, hdiv=hdiv, vdiv=vdiv) for i in training])\n",
    "testing_features = np.array([lbp_features(i, hdiv=hdiv, vdiv=vdiv) for i in testing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 2048), (300, 2047))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_features_indexes = clean(training_features)\n",
    "cleaned_features = training_features[:, cleaned_features_indexes]\n",
    "cleaned_testing_features = testing_features[:, cleaned_features_indexes] # mismo clean para testing\n",
    "training_features.shape, cleaned_features.shape # se eliminó solo una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_features, norm_a, norm_b = normalize(cleaned_features)\n",
    "normalized_testing_features = cleaned_testing_features * norm_a + norm_b # misma normalización para testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selecting Features: 100%|██████████| 26.0/26.0 [03:31<00:00, 8.14s/ features]\n"
     ]
    }
   ],
   "source": [
    "N_FEATURES = 40\n",
    "selected_features_indexes = sfs(normalized_features, y_training, n_features=N_FEATURES, method=\"fisher\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar las features seleccionadas inmediatamente tras el SFS con 26 features entrega peores resultados que con menos features, de hecho el resultado que mostraré a continuación utiliza solo 24 features, sin embargo, esas 24 características no se obtienen de un SFS de N=24 por lo que empíricamente lo que se probó fue con combinaciones que dieran el mayor rendimiento.\n",
    "\n",
    "Se computaron algunas combinaciones de las features seleccionadas por el SFS tales que maximizaran el resultado, considerando de ante mano que en muchas pruebas se alcanzaba el 100%. Para esto se utilizaron combinaciones de largo i-2 para cada i entre 0 y el tamaño del SFS realizado, por lo que el índice con el que se finaliza es el 26, para una combinación de largo 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "for i in range(20, N_FEATURES + 1):\n",
    "    s = []\n",
    "    indexes = combinations(selected_features_indexes[:i], i - 2)\n",
    "    for ixs in indexes:\n",
    "        sf = normalized_features[:, ixs]\n",
    "        n = KNeighborsClassifier(n_neighbors=1)\n",
    "        n.fit(sf, y_training)\n",
    "        ctf = testing_features[:, cleaned_features_indexes]\n",
    "        ntf = ctf * norm_a + norm_b\n",
    "        stf = ntf[:, ixs]\n",
    "        yp = n.predict(stf)\n",
    "        corr = (yp == y_testing).astype(int).sum()\n",
    "        acc = corr / y_testing.size * 100\n",
    "        s.append([acc, ixs])\n",
    "    best_selected_indexes = max(s)[1]\n",
    "    if max(s)[0] == 100:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa se detiene anteriormente para evitar seguir si ya se consiguió un 100%, y esto se da con 24 características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 1851, 1726, 1595, 1655, 1144, 1275, 737, 1775, 1280, 391, 1670, 251, 1015, 632, 568, 511, 248, 286, 707, 195, 387, 763, 1795)\n"
     ]
    }
   ],
   "source": [
    "print(best_selected_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de no querer correr el código anterior, considerando el tiempo de demora del SFS de 30 features, se puede descomentar la siguiente linea, que es el resultado de lo obtenido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_selected_indexes = (376, 1851, 1726, 1595, 1655, 1144, 1275, 737, 1775, 1280, 391, 1670, 251, 1015, 632, 568, 511, 248, 286, 707, 195, 387, 763, 1795)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar el rendimiento (es lo mismo que se hace en cada iteración del _for_ interior del código anterior) se indexa la matriz de imagenes limpiada y normalizada con esos índices seleccionados, tanto para training como para testing, se entrena el clasificador de KNN y finalmente se evalúa el rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = normalized_features[:, best_selected_indexes]\n",
    "selected_testing_features = normalized_testing_features[:, best_selected_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = KNeighborsClassifier(n_neighbors=1)\n",
    "neighbors.fit(selected_features, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = neighbors.predict(selected_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = (y_prediction == y_testing).astype(int).sum()\n",
    "accuracy = correct / y_testing.size * 100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte, el procedimiento es muy similar a lo anterior, cargando las pares en vez de las impares, pero utilizando _best_selected_indexes_ que fue obtenido en el experimento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_training_names = [i for i in even_faces_names if int(i[9:11]) > 1]\n",
    "even_testing_names = [i for i in even_faces_names if int(i[9:11]) <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_training = [imread(f'faces_ARLQ/{i}') for i in even_training_names]\n",
    "even_y_training = np.array([int(i[5:8]) // 2 for i in even_training_names])\n",
    "even_testing = [imread(f'faces_ARLQ/{i}') for i in even_testing_names]\n",
    "even_y_testing = np.array([int(i[5:8]) // 2 for i in even_testing_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_training_features = np.array([lbp_features(i, hdiv=hdiv, vdiv=vdiv) for i in even_training])\n",
    "even_testing_features = np.array([lbp_features(i, hdiv=hdiv, vdiv=vdiv) for i in even_testing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_cleaned_training_features = even_training_features[:, cleaned_features_indexes]\n",
    "even_cleaned_testing_features = even_testing_features[:, cleaned_features_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_normalized_training_features, even_norm_a, even_norm_b = normalize(even_cleaned_training_features)\n",
    "even_normalized_testing_features = even_cleaned_testing_features * even_norm_a + even_norm_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_selected_training_features = even_normalized_training_features[:, best_selected_indexes]\n",
    "even_selected_testing_features = even_normalized_testing_features[:, best_selected_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(even_selected_training_features, even_y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_y_prediction = knn.predict(even_selected_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_correct = (even_y_prediction == even_y_testing).astype(int).sum()\n",
    "even_accuracy = even_correct / even_y_testing.size * 100\n",
    "even_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarmente, como se observa, el rendimiento es perfecto con esos índices, y se obtiene un 100% en ambos experimentos, utilizando las features seleccionadas a partir de la parte impar del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
